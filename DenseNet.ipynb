{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DenseNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5ULPo8ZaMzk",
        "colab_type": "text"
      },
      "source": [
        "# Efficient DenseNet PyTorch\n",
        "Implementation is based on [**efficient_densenet_pytorch** by **gpleiss**](https://github.com/gpleiss/efficient_densenet_pytorch).\n",
        "\n",
        "Edited and converted to Jupyter Notebook by [**Vinh Quang Tran** a.k.a **vinhtq115**](https://github.com/vinhtq115).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzRfRyoUagCc",
        "colab_type": "text"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpewd5DeZMc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint as cp\n",
        "import os\n",
        "import time\n",
        "from google.colab import drive # For saving model and checkpoint to Google Drive\n",
        "from torchvision import datasets, transforms\n",
        "from collections import OrderedDict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh_tFWWQa158",
        "colab_type": "text"
      },
      "source": [
        "## Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flfQOcw1a1Xn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQ0Nk_iqbalW",
        "colab_type": "text"
      },
      "source": [
        "## Set save directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wv_PGmGbhoh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# These are 3 models in Table 3 of paper\n",
        "save_dir_DNBC_100_12 = '/content/gdrive/My Drive/original_implementation/DNBC_100_12'\n",
        "save_dir_DNBC_250_24 = '/content/gdrive/My Drive/original_implementation/DNBC_250_24'\n",
        "save_dir_DNBC_190_40 = '/content/gdrive/My Drive/original_implementation/DNBC_190_40'\n",
        "# Location to CIFAR-10 dataset\n",
        "cifar10='/content/gdrive/My Drive/cifar10/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkAr8DlPxWJ3",
        "colab_type": "text"
      },
      "source": [
        "## Get GPU info\n",
        "Just to make sure Google Colab Pro is working properly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJh20-3PxTqp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime â†’ \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQAci5Ktbty6",
        "colab_type": "text"
      },
      "source": [
        "## Define model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__WnfXrxb-aS",
        "colab_type": "text"
      },
      "source": [
        "### Bottleneck function factory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yiPbukpbvuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _bn_function_factory(norm, relu, conv):\n",
        "    # Bottleneck layer\n",
        "    # Reduce the number of input feature-maps, improve computational efficiency\n",
        "    def bn_function(*inputs):\n",
        "        concated_features = torch.cat(inputs, 1)\n",
        "        bottleneck_output = conv(relu(norm(concated_features)))\n",
        "        return bottleneck_output\n",
        "\n",
        "    return bn_function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiqP7LK6ci4b",
        "colab_type": "text"
      },
      "source": [
        "### DenseLayer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6W80OL5hckS9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class _DenseLayer(nn.Module):\n",
        "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, efficient=False):\n",
        "        super(_DenseLayer, self).__init__()\n",
        "        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),\n",
        "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
        "        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size * growth_rate,\n",
        "                        kernel_size=1, stride=1, bias=False)),\n",
        "        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),\n",
        "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
        "        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n",
        "                        kernel_size=3, stride=1, padding=1, bias=False)),\n",
        "        self.drop_rate = drop_rate\n",
        "        self.efficient = efficient\n",
        "\n",
        "    def forward(self, *prev_features):\n",
        "        bn_function = _bn_function_factory(self.norm1, self.relu1, self.conv1)\n",
        "        if self.efficient and any(prev_feature.requires_grad for prev_feature in prev_features):\n",
        "            bottleneck_output = cp.checkpoint(bn_function, *prev_features)\n",
        "        else:\n",
        "            bottleneck_output = bn_function(*prev_features)\n",
        "        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n",
        "        if self.drop_rate > 0:\n",
        "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
        "        return new_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvE7xrC3cnEB",
        "colab_type": "text"
      },
      "source": [
        "### Transition Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxDa5dZYcqIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class _Transition(nn.Sequential):\n",
        "    def __init__(self, num_input_features, num_output_features):\n",
        "        super(_Transition, self).__init__()\n",
        "        self.add_module('norm', nn.BatchNorm2d(num_input_features))\n",
        "        self.add_module('relu', nn.ReLU(inplace=True))\n",
        "        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,\n",
        "                                          kernel_size=1, stride=1, bias=False))\n",
        "        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cMIIWC8ctJa",
        "colab_type": "text"
      },
      "source": [
        "### DenseBlock"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HdqC0tPcuUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class _DenseBlock(nn.Module):\n",
        "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate, efficient=False):\n",
        "        super(_DenseBlock, self).__init__()\n",
        "        for i in range(num_layers):\n",
        "            layer = _DenseLayer(\n",
        "                num_input_features + i * growth_rate,\n",
        "                growth_rate=growth_rate,\n",
        "                bn_size=bn_size,\n",
        "                drop_rate=drop_rate,\n",
        "                efficient=efficient,\n",
        "            )\n",
        "            self.add_module('denselayer%d' % (i + 1), layer)\n",
        "\n",
        "    def forward(self, init_features):\n",
        "        features = [init_features]\n",
        "        for name, layer in self.named_children():\n",
        "            new_features = layer(*features)\n",
        "            features.append(new_features)\n",
        "        return torch.cat(features, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVuPz1fYcw75",
        "colab_type": "text"
      },
      "source": [
        "### DenseNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CSqWOe0c3kT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DenseNet(nn.Module):\n",
        "    r\"\"\"Densenet-BC model class, based on\n",
        "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`\n",
        "    Args:\n",
        "        growth_rate (int) - how many filters to add each layer (`k` in paper)\n",
        "        block_config (list of 3 or 4 ints) - how many layers in each pooling block\n",
        "        num_init_features (int) - the number of filters to learn in the first convolution layer\n",
        "        bn_size (int) - multiplicative factor for number of bottle neck layers\n",
        "            (i.e. bn_size * k features in the bottleneck layer)\n",
        "        drop_rate (float) - dropout rate after each dense layer\n",
        "        num_classes (int) - number of classification classes\n",
        "        small_inputs (bool) - set to True if images are 32x32. Otherwise assumes images are larger.\n",
        "        efficient (bool) - set to True to use checkpointing. Much more memory efficient, but slower.\n",
        "    \"\"\"\n",
        "    def __init__(self, name, growth_rate=12, block_config=(16, 16, 16), compression=0.5,\n",
        "                 num_init_features=24, bn_size=4, drop_rate=0,\n",
        "                 num_classes=10, small_inputs=True, efficient=False):\n",
        "\n",
        "        super(DenseNet, self).__init__()\n",
        "        assert 0 < compression <= 1, 'compression of densenet should be between 0 and 1'\n",
        "        \n",
        "        self.name = name\n",
        "        self.avgpool_size = 8 if small_inputs else 7\n",
        "\n",
        "        # First convolution\n",
        "        if small_inputs:\n",
        "            self.features = nn.Sequential(OrderedDict([\n",
        "                ('conv0', nn.Conv2d(3, num_init_features, kernel_size=3, stride=1, padding=1, bias=False)),\n",
        "            ]))\n",
        "        else:\n",
        "            self.features = nn.Sequential(OrderedDict([\n",
        "                ('conv0', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n",
        "            ]))\n",
        "            self.features.add_module('norm0', nn.BatchNorm2d(num_init_features))\n",
        "            self.features.add_module('relu0', nn.ReLU(inplace=True))\n",
        "            self.features.add_module('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1,\n",
        "                                                           ceil_mode=False))\n",
        "\n",
        "        # Each denseblock\n",
        "        num_features = num_init_features\n",
        "        for i, num_layers in enumerate(block_config):\n",
        "            block = _DenseBlock(\n",
        "                num_layers=num_layers,\n",
        "                num_input_features=num_features,\n",
        "                bn_size=bn_size,\n",
        "                growth_rate=growth_rate,\n",
        "                drop_rate=drop_rate,\n",
        "                efficient=efficient,\n",
        "            )\n",
        "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
        "            num_features = num_features + num_layers * growth_rate\n",
        "            if i != len(block_config) - 1:\n",
        "                trans = _Transition(num_input_features=num_features,\n",
        "                                    num_output_features=int(num_features * compression))\n",
        "                self.features.add_module('transition%d' % (i + 1), trans)\n",
        "                num_features = int(num_features * compression)\n",
        "\n",
        "        # Final batch norm\n",
        "        self.features.add_module('norm_final', nn.BatchNorm2d(num_features))\n",
        "\n",
        "        # Linear layer\n",
        "        self.classifier = nn.Linear(num_features, num_classes)\n",
        "\n",
        "        # Initialization\n",
        "        for name, param in self.named_parameters():\n",
        "            if 'conv' in name and 'weight' in name:\n",
        "                n = param.size(0) * param.size(2) * param.size(3)\n",
        "                param.data.normal_().mul_(math.sqrt(2. / n))\n",
        "            elif 'norm' in name and 'weight' in name:\n",
        "                param.data.fill_(1)\n",
        "            elif 'norm' in name and 'bias' in name:\n",
        "                param.data.fill_(0)\n",
        "            elif 'classifier' in name and 'bias' in name:\n",
        "                param.data.fill_(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.features(x)\n",
        "        out = F.relu(features, inplace=True)\n",
        "        out = F.avg_pool2d(out, kernel_size=self.avgpool_size).view(features.size(0), -1)\n",
        "        #out = F.adaptive_avg_pool2d(out, (1, 1))\n",
        "        #out = torch.flatten(out, 1)\n",
        "        out = self.classifier(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQE7kJnrdB2o",
        "colab_type": "text"
      },
      "source": [
        "### Template model definition function\n",
        "[**efficient_densenet_pytorch** by **gpleiss**](https://github.com/gpleiss/efficient_densenet_pytorch) has different number of parameters for DenseNet-BC_250_24 and DenseNet-BC_190_40 from the paper. This is because it keep ``num_init_features`` at 24 instead of twice the ``growth_rate`` for all models and as a result, only the DenseNet-BC_100_12 have the same number of parameters (after rounded up).\n",
        "\n",
        "This template will ensure that the numbers matches with those in the paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlZ5k2nNdH34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def denseNetBC_100_12(eff = False):  # Growth rate 12, depth 100\n",
        "    return DenseNet('DenseNet-BC_12_100', 12, (16,16,16), 0.5, 24, 4, 0, 10, efficient=eff)\n",
        "\n",
        "def denseNetBC_250_24(eff = False):  # Growth rate 24, depth 250\n",
        "    return DenseNet('DenseNet-BC_24_250', 24, (41, 41, 41), 0.5, 48, 4, 0, 10, efficient=eff)\n",
        "\n",
        "def denseNetBC_190_40(eff = False):  # Growth rate 40, depth 190\n",
        "    return DenseNet('DenseNet-BC_40_190', 40, (31,31,31), 0.5, 80, 4, 0, 10, efficient=eff)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpq0eo5DddML",
        "colab_type": "text"
      },
      "source": [
        "### Average Meter\n",
        "For calculating average."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZVz3dfkdft_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUg9oXDQdjWu",
        "colab_type": "text"
      },
      "source": [
        "## Train epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phY0Bg11dj9-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_epoch(model, loader, optimizer, epoch, n_epochs, print_freq=1):\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    error = AverageMeter()\n",
        "\n",
        "    # Model on train mode\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    for batch_idx, (input, target) in enumerate(loader):\n",
        "        # Create vaiables\n",
        "        if torch.cuda.is_available():\n",
        "            input = input.cuda()\n",
        "            target = target.cuda()\n",
        "\n",
        "        # compute output\n",
        "        output = model(input)\n",
        "        loss = torch.nn.functional.cross_entropy(output, target)\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        batch_size = target.size(0)\n",
        "        _, pred = output.data.cpu().topk(1, dim=1)\n",
        "        error.update(torch.ne(pred.squeeze(), target.cpu()).float().sum().item() / batch_size, batch_size)\n",
        "        losses.update(loss.item(), batch_size)\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        # print stats\n",
        "        if batch_idx % print_freq == 0:\n",
        "            res = '\\t'.join([\n",
        "                'Epoch: [%d/%d]' % (epoch + 1, n_epochs),\n",
        "                'Iter: [%d/%d]' % (batch_idx + 1, len(loader)),\n",
        "                'Time %.3f (%.3f)' % (batch_time.val, batch_time.avg),\n",
        "                'Loss %.5f (%.5f)' % (losses.val, losses.avg),\n",
        "                'Error %.5f (%.5f)' % (error.val, error.avg),\n",
        "            ])\n",
        "            print(res)\n",
        "\n",
        "    # Return summary statistics\n",
        "    return batch_time.avg, losses.avg, error.avg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFzKSnu1dqbk",
        "colab_type": "text"
      },
      "source": [
        "## Test epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlwMtnfkduld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_epoch(model, loader, print_freq=1, is_test=True):\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    error = AverageMeter()\n",
        "\n",
        "    # Model on eval mode\n",
        "    model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (input, target) in enumerate(loader):\n",
        "            # Create vaiables\n",
        "            if torch.cuda.is_available():\n",
        "                input = input.cuda()\n",
        "                target = target.cuda()\n",
        "\n",
        "            # compute output\n",
        "            output = model(input)\n",
        "            loss = torch.nn.functional.cross_entropy(output, target)\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            batch_size = target.size(0)\n",
        "            _, pred = output.data.cpu().topk(1, dim=1)\n",
        "            error.update(torch.ne(pred.squeeze(), target.cpu()).float().sum().item() / batch_size, batch_size)\n",
        "            losses.update(loss.item(), batch_size)\n",
        "\n",
        "            # measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            # print stats\n",
        "            if batch_idx % print_freq == 0:\n",
        "                res = '\\t'.join([\n",
        "                    'Test' if is_test else 'Valid',\n",
        "                    'Iter: [%d/%d]' % (batch_idx + 1, len(loader)),\n",
        "                    'Time %.3f (%.3f)' % (batch_time.val, batch_time.avg),\n",
        "                    'Loss %.5f (%.5f)' % (losses.val, losses.avg),\n",
        "                    'Error %.5f (%.5f)' % (error.val, error.avg),\n",
        "                ])\n",
        "                print(res)\n",
        "\n",
        "    # Return summary statistics\n",
        "    return batch_time.avg, losses.avg, error.avg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hru_RUdPfk-v",
        "colab_type": "text"
      },
      "source": [
        "## Train function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zemnow3vfml_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, train_set, valid_set, test_set, save, n_epochs=300,\n",
        "          batch_size=64, lr=0.1, wd=0.0001, momentum=0.9, seed=None):\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "    # Data loaders\n",
        "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True,\n",
        "                                               pin_memory=(torch.cuda.is_available()), num_workers=0)\n",
        "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False,\n",
        "                                              pin_memory=(torch.cuda.is_available()), num_workers=0)\n",
        "    if valid_set is None:\n",
        "        valid_loader = None\n",
        "    else:\n",
        "        valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, shuffle=False,\n",
        "                                                   pin_memory=(torch.cuda.is_available()), num_workers=0)\n",
        "            \n",
        "    # Model on cuda\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "\n",
        "    # Wrap model for multi-GPUs, if necessary\n",
        "    model_wrapper = model\n",
        "    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
        "        model_wrapper = torch.nn.DataParallel(model).cuda()\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.SGD(model_wrapper.parameters(), lr=lr, momentum=momentum, nesterov=True, weight_decay=wd)\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[0.5 * n_epochs, 0.75 * n_epochs],\n",
        "                                                    gamma=0.1)\n",
        "\n",
        "    # Start log\n",
        "    if not os.path.isfile(os.path.join(save, 'results.csv')):\n",
        "        with open(os.path.join(save, 'results.csv'), 'w') as f:\n",
        "            f.write('epoch,train_loss,train_error,valid_loss,valid_error,test_error\\n')\n",
        "\n",
        "    if os.path.isfile(os.path.join(save, 'checkpoint.pth')):\n",
        "        _checkpoint = torch.load(os.path.join(save, 'checkpoint.pth'))\n",
        "        model.load_state_dict(_checkpoint['model'])\n",
        "        model_wrapper.load_state_dict(_checkpoint['model_wrapper'])\n",
        "        lr = _checkpoint['lr']\n",
        "        optimizer.load_state_dict(_checkpoint['optimizer'])\n",
        "        scheduler.load_state_dict(_checkpoint['scheduler'])\n",
        "        for g in optimizer.param_groups:\n",
        "            g['lr'] = lr\n",
        "        best_error = _checkpoint['best_error']\n",
        "        start_epoch = _checkpoint['current_epoch'] + 1\n",
        "    else:\n",
        "        best_error = 1\n",
        "        start_epoch = 0\n",
        "    \n",
        "    # Train model\n",
        "    for epoch in range(start_epoch, n_epochs):\n",
        "        # Reduce learning rate\n",
        "        if epoch == 149 or epoch == 224:\n",
        "            lr = lr / 10\n",
        "            for g in optimizer.param_groups:\n",
        "                g['lr'] = lr\n",
        "\n",
        "        _, train_loss, train_error = train_epoch(\n",
        "            model=model_wrapper,\n",
        "            loader=train_loader,\n",
        "            optimizer=optimizer,\n",
        "            epoch=epoch,\n",
        "            n_epochs=n_epochs,\n",
        "            print_freq=10,\n",
        "        )\n",
        "        scheduler.step()\n",
        "        _, valid_loss, valid_error = test_epoch(\n",
        "            model=model_wrapper,\n",
        "            loader=valid_loader if valid_loader else test_loader,\n",
        "            is_test=(not valid_loader),\n",
        "            print_freq=10\n",
        "        )\n",
        "\n",
        "        # Determine if model is the best\n",
        "        if valid_loader:\n",
        "            if valid_error < best_error:\n",
        "                best_error = valid_error\n",
        "                print('New best error: %.4f' % best_error)\n",
        "                torch.save(model.state_dict(), os.path.join(save, 'model.dat'))\n",
        "        else:\n",
        "            torch.save(model.state_dict(), os.path.join(save, 'model.dat'))\n",
        "\n",
        "        checkpoint= {\n",
        "                    'epoch': epoch,\n",
        "                    'best_error': best_error,\n",
        "                    'model': model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'scheduler': scheduler.state_dict(),\n",
        "                    'lr': lr,\n",
        "                    'model_wrapper': model_wrapper.state_dict(),\n",
        "                    'current_epoch': epoch\n",
        "                }\n",
        "        torch.save(checkpoint, os.path.join(save, 'checkpoint.pth'))\n",
        "\n",
        "        # Log results\n",
        "        with open(os.path.join(save, 'results.csv'), 'a') as f:\n",
        "            f.write('%03d,%0.6f,%0.6f,%0.5f,%0.5f,\\n' % (\n",
        "                (epoch + 1),\n",
        "                train_loss,\n",
        "                train_error,\n",
        "                valid_loss,\n",
        "                valid_error,\n",
        "            ))\n",
        "\n",
        "    # Final test of model on test set\n",
        "    model.load_state_dict(torch.load(os.path.join(save, 'model.dat')))\n",
        "    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
        "        model = torch.nn.DataParallel(model).cuda()\n",
        "    test_results = test_epoch(\n",
        "        model=model,\n",
        "        loader=test_loader,\n",
        "        is_test=True\n",
        "    )\n",
        "    _, _, test_error = test_results\n",
        "    with open(os.path.join(save, 'results.csv'), 'a') as f:\n",
        "        f.write(',,,,,%0.5f\\n' % (test_error))\n",
        "    print('Final test error: %.5f' % test_error)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buvfAB3HfufI",
        "colab_type": "text"
      },
      "source": [
        "## Demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEaGm-SAf9Lz",
        "colab_type": "text"
      },
      "source": [
        "### Demo function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTDzqFWqfxJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def demo(data, save, depth=100, growth_rate=12, efficient=True, valid_size=5000,\n",
        "         n_epochs=300, batch_size=64, seed=None):\n",
        "    \"\"\"\n",
        "    A demo to show off training of efficient DenseNets.\n",
        "    Trains and evaluates a DenseNet-BC on CIFAR-10.\n",
        "    Args:\n",
        "        data (str) - path to directory where data should be loaded from/downloaded\n",
        "            (default $DATA_DIR)\n",
        "        save (str) - path to save the model to (default /tmp)\n",
        "        depth (int) - depth of the network (number of convolution layers) (default 40)\n",
        "        growth_rate (int) - number of features added per DenseNet layer (default 12)\n",
        "        efficient (bool) - use the memory efficient implementation? (default True)\n",
        "        valid_size (int) - size of validation set\n",
        "        n_epochs (int) - number of epochs for training (default 300)\n",
        "        batch_size (int) - size of minibatch (default 256)\n",
        "        seed (int) - manually set the random seed (default None)\n",
        "    \"\"\"\n",
        "\n",
        "    # Data transforms\n",
        "    # For racing purpose, we will use data augmentation. RACING IS FUN!!!\n",
        "    mean = [0.5071, 0.4867, 0.4408]\n",
        "    stdv = [0.2675, 0.2565, 0.2761]\n",
        "    train_transforms = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=stdv),\n",
        "    ])\n",
        "    test_transforms = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=stdv),\n",
        "    ])\n",
        "\n",
        "    # Datasets\n",
        "    train_set = datasets.CIFAR10(data, train=True, transform=train_transforms, download=True)\n",
        "    test_set = datasets.CIFAR10(data, train=False, transform=test_transforms, download=False)\n",
        "\n",
        "    if valid_size:\n",
        "        valid_set = datasets.CIFAR10(data, train=True, transform=test_transforms)\n",
        "        indices = torch.randperm(len(train_set))\n",
        "        train_indices = indices[:len(indices) - valid_size]\n",
        "        valid_indices = indices[len(indices) - valid_size:]\n",
        "        train_set = torch.utils.data.Subset(train_set, train_indices)\n",
        "        valid_set = torch.utils.data.Subset(valid_set, valid_indices)\n",
        "    else:\n",
        "        valid_set = None\n",
        "\n",
        "    # Models\n",
        "    if depth == 100 and growth_rate == 12:\n",
        "        model = denseNetBC_100_12(eff=efficient)\n",
        "    elif depth == 250 and growth_rate == 24:\n",
        "        model = denseNetBC_250_24(eff=efficient)\n",
        "    elif depth == 190 and growth_rate == 40:\n",
        "        model = denseNetBC_190_40(eff=efficient)\n",
        "    else:\n",
        "        # Get densenet configuration\n",
        "        if (depth - 4) % 3:\n",
        "            raise Exception('Invalid depth')\n",
        "        block_config = [(depth - 4) // 6 for _ in range(3)]\n",
        "        model = DenseNet(name='DenseNet', \n",
        "                         growth_rate=growth_rate, \n",
        "                         block_config=block_conf, \n",
        "                         compression=0.5,\n",
        "                         num_init_features=growth_rate*2, \n",
        "                         bn_size=4, \n",
        "                         drop_rate=0,\n",
        "                         num_classes=10,\n",
        "                         small_inputs=True,\n",
        "                         efficient=efficient)\n",
        "\n",
        "    # print(model)\n",
        "    num_params = sum(p.numel() for p in model.parameters())\n",
        "    print(\"Total parameters: \", num_params)\n",
        "\n",
        "    # Make save directory\n",
        "    if not os.path.exists(save):\n",
        "        os.makedirs(save)\n",
        "    if not os.path.isdir(save):\n",
        "        raise Exception('%s is not a dir' % save)\n",
        "\n",
        "    # Train the model\n",
        "    train(model=model, train_set=train_set, valid_set=valid_set, test_set=test_set, save=save,\n",
        "          n_epochs=n_epochs, batch_size=batch_size, seed=seed)\n",
        "    print('Done!')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgEPBcoO8NxF",
        "colab_type": "text"
      },
      "source": [
        "### DenseNet-BC depth=100 growth_rate=12"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN9p9jRW8VAy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "demo(data=cifar10, save=save_dir_DNBC_100_12, depth=100, growth_rate=12, efficient=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7WZcWc68ae_",
        "colab_type": "text"
      },
      "source": [
        "### DenseNet-BC depth=250 growth_rate=24"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gwq15-M98fax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "demo(data=cifar10, save=save_dir_DNBC_250_24, depth=250, growth_rate=24, efficient=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-8toPEZ84YO",
        "colab_type": "text"
      },
      "source": [
        "### DenseNet-BC depth=190 growth_rate=40"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkEvilUx88FZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "demo(data=cifar10, save=save_dir_DNBC_190_40, depth=190, growth_rate=40, efficient=True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}